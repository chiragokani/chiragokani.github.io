<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Useful identities</title>
   <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
   </script>
<script type="text/javascript">
window.MathJax = {
  tex: {
    tags: 'ams',
    macros: {
    Order: "{\\mathcal O}",
    vec: ['\\mathbf{#1}',1],
    dyad: ['\\boldsymbol{\\mathsf{#1}}',1],
    divergence: "{\\boldsymbol{\\nabla}\\cdot\\,}",
    gradient: "{\\boldsymbol\\nabla}",
    curl: "{\\boldsymbol\\nabla\\times\\,}",
    Laplacian: "{\\nabla^2}",
    Laplacianperp: "{\\nabla_{\\!\\!\\perp}^2}"
    }
  }
};
</script>	
<link rel="stylesheet" href="..\..\chirag.css">
</head>
<body>

<h2 id="ids">Useful identities</h2>
<p>Identities (1) and (3) are proved in general curvilinear coordinates&mdash;see Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>. For a less general (but more straightforward) treatment, these identities are also proved in index notation for orthonormal coordinates. Note that the outer product is defined as \((\vec{u} \otimes \vec{v}) \cdot \vec{w} = (\vec{v} \cdot \vec{w}) \vec{u}\).</p>
<ol>
<li><p>\(\divergence(\vec{u}\otimes\vec{v})=(\gradient \vec{u})\vec{v}+ (\divergence \vec{v})\vec{u}\). This product rule is shown below in general curvilinear coordinates, where \(\vec{g}_k\) is a contravariant basis vector, and where \(_{,k}\) denotes differentiation with respect to the the \(k^\mathrm{th}\) coordinate in the basis, and where \(\vec{u} = u^k \vec{g}_k\) means \(\sum_{k=1}^3 u^k \vec{g}_k = u^1 \vec{g}_1 + u^2 \vec{g}_2 + u^3 \vec{g}_3\). Evaluating the left-hand side of the identity to be proved yields
\begin{align*}
\divergence(\vec{u}\otimes\vec{v})&= (\vec{u}\otimes\vec{v})_{,k}\cdot \vec{g}^k\\	
&= (\vec{u}_{,k}\otimes \vec{v} + \vec{u}\otimes \vec{v}_{,k}) \cdot  \vec{g}^k \\
&= (\vec{u}_{,k}\otimes \vec{v}) \cdot \vec{g}^k + (\vec{u}\otimes \vec{v}_{,k}) \cdot \vec{g}^k \\
&=(\vec{u}_{,k}\otimes\vec{g}^k) \cdot \vec{v} + (\vec{v}_{,k}\cdot \vec{g}^k)  \vec{u} \\
&=(\gradient\vec{u}) \cdot \vec{v} + (\divergence\vec{v})\vec{u}\,,
\end{align*}
where the first line holds by the definition of divergence of tensor, the second line by the product rule, the third line by the distributive property, the fourth line by the definition of the outer product, and the last line by the definitions of the gradient and divergence of a vector. <i>Note: This was a homework problem in Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>.</i></p>

<p>Below is a less general version of the proof, included for readers unfamiliar with Ricci calculus. In orthonormal index notation, the left-hand side of the identity to be proved in one line, by the product rule:
\begin{align*}
(u_iv_j)_{,j} &= u_{i,j} v_j + u_i v_{j,j}\,.
\end{align*}
</p>
</li>

<li>\(\langle \partial (fg)/\partial t\rangle = 0\) for time-harmonic \(f(t)\) and \(g(t)\). 
To show this, suppose \(f(t) = \cos \omega t\) and \(g(t) = \cos(\omega t + \phi)\). Then, since \([\cos(a+b) + \cos(a-b)]/2 = \cos a\cos b\),
\[f(t)g(t) = \tfrac{1}{2}[\cos(2\omega t + \phi) + \cos \phi]\,.\]
Taking the time derivative of the product yields
\begin{align}\label{eq:id:fg}
\frac{d}{dt}[f(t)g(t)] = -\omega \sin(2\omega t + \phi). 
\end{align}
The time-average of \(h(t)\) having angular frequency \(\omega = 2\pi f = 2\pi/\tau\), where \(\tau\) is the period, is defined as 
\[\langle h(t)\rangle = \frac{1}{\tau} \int_{0}^{\tau} h(t) \,dt\,.\]
Thus the time average of Eq. \eqref{eq:id:fg} is
\begin{align*}
\left\langle\frac{d}{dt}[f(t)g(t)]\right\rangle &= -\langle\omega \sin(2\omega t + \phi)\rangle\\
&=-\frac{\omega}{\tau} \int_0^\tau \sin (2\omega t +\phi) \,dt\\
&= \frac{\omega}{\tau} \frac{1}{2\omega}\cos (2\omega t + \phi)\bigg\rvert_{0}^\tau\\
&= \frac{1}{2\tau} [\cos (4\pi + \phi) - \cos\phi] = 0
\end{align*}
since \(\cos(4\pi + \phi) = \cos \phi\). Thus, at least for time-harmonic functions, \(\langle{d}[f(t)g(t)]/{dt}\rangle = 0\). Since the theory on this website only involves time-harmonic functions, this property will be exploited when deriving the acoustic radiation stress tensor.
</li>


<li>
<p>
\(\divergence(\phi\dyad{T})= \phi\,\divergence\dyad{T}+ \dyad{T}\cdot (\gradient \phi)\), where rank-2 tensors on this website are denoted by being underlined. The left-hand side of the identity is manipulated:
\begin{align*}
\divergence(\phi\dyad{T}) &= (\phi\dyad{T})_{,k} \cdot \vec{g}^k \\
&= \phi\dyad{T}_{,k} \cdot \vec{g}^k + \phi_{,k}\dyad{T}\cdot \vec{g}^k \\
&= \phi\dyad{T}_{,k} \cdot \vec{g}^k + \dyad{T} \cdot (\phi_{,k} \vec{g}^k) \\
&= \phi\,\divergence\dyad{T}+ \dyad{T} \cdot (\gradient \phi)\,.
\end{align*}
The first line holds by the definition of the divergence of a tensor, the second line holds by the product rule, the third line by the commutative property of multiplication, and the fourth line by the definition of the divergence of a tensor and vector. 
</p>
<p>
For a less general version of this proof, appeal to orthonormal index notation. As in Item 1, the statement is proved in one line by the product rule:
\begin{align}
(\phi T_{ij})_{,j} = \phi T_{ij,j} + T_{ij} \phi_{,j}  
\end{align}
</p>

</li>

<li>
<p>For two time-harmonic functions \(f\) and \(g\) represented by the real parts of the complex-valued functions \(\tilde{f}(t) = f_0 e^{-i(\omega t + \phi_f)} = \tilde{f}_\omega e^{-i\omega t}\) and \(\tilde{g}(t) = g_0 e^{-i(\omega t + \phi_g)} = \tilde{g}_\omega e^{-i\omega t}\) (where \(\tilde{f}_\omega = f_0e^{-i\phi_F}\) and \(\tilde{g}_\omega = g_0 e^{-i\phi_G}\)), the time average of their product, \(\langle fg\rangle\), is given by \(\langle \Re (\tilde{f}) \,\,\Re (\tilde{g}) \rangle = \frac{1}{2} \Re (\tilde{f}_\omega \, \tilde{g}^*_\omega) = \frac{1}{2} \Re (\tilde{f}_\omega^*\, \tilde{g}_\omega)\), where "\(\Re\)" denotes "real part".</p>

<p>To show this, note that \(\Re(\tilde{f}) = f_0 \cos(\omega t + \phi_f)\) and \(\Re(\tilde{g}) = g_0 \cos(\omega t + \phi_g)\). thus
\begin{align}
\langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &= \langle f_0 \cos(\omega t + \phi_f) \, g_0 \cos(\omega t + \phi_g)\rangle  \notag\\
&= f_0 g_0 \langle\cos(\omega t + \phi_f) \,  \cos(\omega t + \phi_g)\rangle\,. \label{eq:id:avg:simplify:1}
\end{align}
Since \(\cos A \cos B = \cos(A+B) + \sin A\sin B\), Eq. \eqref{eq:id:avg:simplify:1} becomes (by letting \(A = \omega t + \phi_f\) and \(B= \omega t + \phi_g\))
\begin{align}
\langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &= f_0 g_0 \langle \cos(2\omega t + \phi_f + \phi_g) +  \sin(\omega t + \phi_f)\sin(\omega t + \phi_g)\rangle\,.\label{eq:id:avg:simplify:2}
\end{align}
Noting that \(\sin A \sin B = \tfrac{1}{2} [\cos(A-B) - \cos (A+B)]\), Eq. \eqref{eq:id:avg:simplify:2} becomes
\begin{align}
\langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &=  f_0 g_0 \langle \cos(2\omega t + \phi_f + \phi_g) - \tfrac{1}{2} \cos(2\omega t + \phi_f + \phi_g) + \tfrac{1}{2}\cos(\phi_f - \phi_g)\rangle \notag\\
&= f_0 g_0 \langle  \tfrac{1}{2} \cos(2\omega t + \phi_f + \phi_g) + \tfrac{1}{2}\cos(\phi_f - \phi_g)\rangle\,.\label{eq:id:avg:simplify:3}
\end{align}
The time-averaging operation amounts to an integral, which is a linear operation. Thus Eq. \eqref{eq:id:avg:simplify:3} becomes
\begin{align*}
\langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &=  \tfrac{1}{2} f_0 g_0 \langle  \cos(2\omega t + \phi_f + \phi_g)\rangle + \tfrac{1}{2} f_0 g_0 \langle\cos(\phi_f - \phi_g)\rangle\,.
\end{align*}
The first term on the left-hand side is 0. Meanwhile, the second term does not depend on time, and therefore its time average is itself: 
\begin{align}
\langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &=  \tfrac{1}{2} f_0 g_0 \cos(\phi_f - \phi_g)\,.\label{eq:id:avg:simplify}
\end{align}
Noting that \(f_0 g_0 \cos(\phi_f - \phi_g) \) is \(\Re [f_0  g_0 e^{-i(\phi_f - \phi_g)}]\), which by the relations \(\tilde{f}_\omega = f_0e^{-i\phi_F}\) and \(\tilde{g}_\omega = g_0 e^{-i\phi_G}\) is \(\Re(\tilde{f}_\omega \, \tilde{g}_\omega^*)\), Eq. \eqref{eq:id:avg:simplify} becomes
\begin{align}
\langle fg \rangle  = \langle \Re (\tilde{f}) \, \Re (\tilde{g}) \rangle &=  \tfrac{1}{2} \Re(\tilde{f}_\omega\tilde{g}_\omega^*) =   \tfrac{1}{2} \Re(\tilde{f}_\omega^*\tilde{g}_\omega) \,,\label{eq:id:avg}
\end{align}
where the final equality holds by noting that \(\cos (\phi_f - \phi_g) = \cos(\phi_g - \phi_f)\).
</p>

<p>
A consequence of this relation is that the time-averaged intensity of a time-harmonic acoustic field is \[\langle \vec{I}\rangle  = \langle p \vec{v} \rangle = \frac{1}{2} \Re(\tilde{p}_\omega \tilde{\vec{v}}_\omega^* ) = \frac{1}{2} \Re(\tilde{p}_\omega ^*\tilde{\vec{v}}_\omega )\,,\] as can be seen by letting \(\Re(\tilde{f}) = p\) and \(\Re(\tilde{\vec{g}})= \vec{v}\) in Eq. \eqref{eq:id:avg}</p>
</li>

<li>\(\gradient \vec{r} = \dyad{I} = (\gradient \vec{r})^\mathrm{T}\), where \(\vec{r}\) is the position vector. Note that the gradient of a vector field \(\vec{f}\) is defined by {See Eq. (1.4.10) of Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>}
\begin{equation}\label{eq:id:grad:vec}
\gradient \vec{f} = \vec{f}_{,k} \otimes \vec{g}^k\,,
\end{equation}
where \(\vec{g}^k\) is the contravariant basis vector. Evaluating Eq. \eqref{eq:id:grad:vec} for \(\vec{r}\) yields 
\begin{equation}\label{eq:id:grad:vec:r}
\gradient \vec{r} = \vec{r}_{,k} \otimes \vec{g}^k\,.
\end{equation}
But \(\vec{g}_k \equiv \vec{r}_{,k}\) {See Eq. (1.3.4) of Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>}, so Eq. \eqref{eq:id:grad:vec:r} becomes
\begin{equation}\label{eq:id:grad:vec:r:2}
\gradient \vec{r} = \vec{g}_k \otimes \vec{g}^k \,.
\end{equation}
The right-hand side of Eq. \eqref{eq:id:grad:vec:r:2} is simply the identity tensor, as is shown by considering an arbitrary vector \(\vec{a}_k\). By the definition of the identity tensor,
\begin{equation}\label{eq:id:I}
\dyad{I}\cdot \vec{a}_k = \vec{a}_k\,. 
\end{equation} 
Dotting Eq. \eqref{eq:id:I} by \(\vec{a}^k\) on both sides results in 
\begin{equation*}
(\dyad{I}\cdot \vec{a}_k) \cdot \vec{a}^k  = \vec{a}_k\cdot \vec{a}^k.
\end{equation*}
Since the basis and its dual are orthogonal,
\begin{equation}\label{eq:id:grad:vec:r:alg}
(\dyad{I}\cdot \vec{a}_k) \cdot \vec{a}^k  = \delta_k^{\,\,k} = 1.
\end{equation}
Multiplying both sides of Eq. \eqref{eq:id:grad:vec:r:alg} by \(\vec{a}_i\) results in
\begin{align}\label{eq:id:grad:vec:3}
[(\dyad{I}\cdot \vec{a}_k) \cdot \vec{a}^k ] \vec{a}_i &= (\vec{a}_k\cdot \vec{a}^k ) \vec{a}_i\,.
\end{align}
In view of Eqs. \eqref{eq:id:grad:vec:r:alg} and \eqref{eq:id:I}, \((\dyad{I}\cdot \vec{a}_k) \cdot \vec{a}^k = \vec{a}^k = \dyad{I} \cdot \vec{a}_k\), so Eq. \eqref{eq:id:grad:vec:3} becomes
\[\dyad{I} \cdot \vec{a}_k = (\vec{a}_k\cdot \vec{a}^k ) \vec{a}_i = (\vec{a}_k\otimes\vec{a}^k) \cdot \vec{a}_i\,,\]
where the definition of the outer product has been used in the second equality. Since \(\vec{a}_i\) is arbitrary,
\begin{equation*}
	\dyad{I}	=\vec{a}_k \otimes\vec{a}^k.
\end{equation*}
Thus \(\vec{g}_k \otimes \vec{g}^k = \dyad{I}\), and Eq. \eqref{eq:id:grad:vec:r:2} becomes 
\begin{equation}\label{eq:id:grad:vec:r:3}
\gradient \vec{r} = \dyad{I}\,,
\end{equation}
proving the first of the two desired equalities. To prove the second of the two equalities, note that for two arbitrary vectors \(\vec{u}\cdot \vec{v}\) [which equals \((\dyad{I}\cdot \vec{u})\cdot \vec{v}\)] equals \(\vec{v}\cdot \vec{u}\) [which equals \((\dyad{I}\cdot \vec{v})\cdot \vec{u}\)], one can use the definition of the transpose to write
\begin{align}\label{eq:id:grad:vec:id}
(\dyad{I}\cdot \vec{u}) \cdot \vec{v} = 
(\dyad{I}^\mathrm{T} \cdot \vec{u}) \cdot \vec{v}\,,
\end{align}
showing that \(\dyad{I} = \dyad{I}^\mathrm{T}\). Thus Eq. \eqref{eq:id:grad:vec:r:3} can be transposed and equated to itself, i.e., \(\gradient \vec{r} = (\gradient \vec{r})^\mathrm{T}\).
</li>

<li>\((\vec{u}\otimes\vec{v})^\mathrm{T} = (\vec{v}\otimes\vec{u})\). Introduce auxiliary vectors \(\vec{a}\) and \(\vec{b}\), and invoke the definition of the transpose: 
\[	[(\vec{u}\otimes\vec{v})^\mathrm{T} \cdot \vec{b}] \cdot\vec{a} = \lbrack(\vec{u}\otimes\vec{v}) \cdot \vec{a}\rbrack \cdot  \vec{b} \,.\]
By the definiton of the outer product, the right-hand side equals \(\lbrack(\vec{v} \cdot \vec{a})\vec{u}\rbrack \cdot \vec{b}\). Since \(\vec{v} \cdot \vec{a}\) is a scalar, it the right-hand side becomes \((\vec{u}\cdot\vec{b})(\vec{v}\cdot\vec{a}) = \lbrack(\vec{u} \cdot \vec{b}) \vec{v}\rbrack\cdot \vec{a}\) by the associative property. Again invoking the definition of the outer product yields
\[ [(\vec{u}\otimes\vec{v})^\mathrm{T} \cdot \vec{b}] \cdot\vec{a}= [(\vec{v}\otimes\vec{u})\cdot \vec{b}] \cdot  \vec{a}\,.\]
Since \(\vec{a}\) and \(\vec{b}\) are arbitrary, it has been shown that \((\vec{u}\otimes\vec{v})^\mathrm{T} = (\vec{v}\otimes\vec{u})\).</li>

<li>
\((\dyad{A} \cdot \dyad{B})^\mathrm{T}= \dyad{B}^\mathrm{T} \cdot \dyad{A}^\mathrm{T}\). This is proved by invoking auxiliary vectors \(\vec{u}\) and \(\vec{v}\) and utilizing the definition of transpose in the first, third, and fourth lines below,
\begin{align*}
[(\dyad{A} \cdot\dyad{B})^\mathrm{T} \cdot \vec{u}] \cdot \vec{v} &= [(\dyad{A} \cdot\dyad{B}) \cdot \vec{v}]\cdot \vec{u} \\ %\tag{definition of transpose}\\
&= [\dyad{A} \cdot(\dyad{B} \cdot\vec{v})]\cdot \vec{u} \\ %\tag{composition property}\\
&= (\dyad{B} \cdot\vec{v})\cdot(\dyad{A}^\mathrm{T} \cdot\vec{u}) \\ %\tag{definition of transpose}\\
&= [\dyad{B}^\mathrm{T} \cdot(\dyad{A}^\mathrm{T} \cdot\vec{u})]\cdot\vec{v}  \\ %\tag{definition of transpose}\\
&= [(\dyad{B}^\mathrm{T} \cdot \dyad{A}^\mathrm{T}) \cdot\vec{u}]\cdot\vec{v}\,, %\tag{composition property}
\end{align*}
where the composition property has been used in the second and final lines above. Since \(\vec{u}\) and \(\vec{v}\) are arbitrary, the proof is complete.
</li>

<li><p>\(\vec{u} \times (\vec{v} \times \vec{w}) = (\vec{u} \cdot \vec{w}) \vec{v} - (\vec{u}\cdot \vec{v}) \vec{w}\), known as the vector triple product. It is straightforward to prove this in index notation for orthonormal bases:
\begin{align}
	\lbrack\vec{u} \times (\vec{v} \times \vec{w})\rbrack_i &= \epsilon_{ijk}u_j(\vec{v}\times\vec{w})_k\notag\\
	&=  \epsilon_{ijk}u_j(\vec{v}\times\vec{w})_k\notag\\
	&=  \epsilon_{ijk}\epsilon_{kmn}u_jv_m w_n\notag\\		
	&=  \epsilon_{kij}\epsilon_{kmn}u_jv_m w_n\notag\\		
	&=  \delta_{im}\delta_{jn}u_jv_m w_n-\delta_{in}\delta_{jm}u_jv_m w_n\notag\\	
	&=  u_nv_i w_n- u_mv_m w_i\notag\\	
	&=  (\vec{u} \cdot \vec{w}) v_i - (\vec{u}\cdot\vec{v}) w_i\notag		
\end{align}
Since this applies to \(i=1,2,3\), the above statement becomes \(\vec{u} \times (\vec{v} \times \vec{w}) = (\vec{u} \cdot \vec{w}) \vec{v} - (\vec{u}\cdot \vec{v}) \vec{w}\), as desired. </p>
</li>


<li><p>Definition of the axial vector \(\vec{w}\) of a skew-symmetric tensor \(\dyad{W}\): \(\vec{w} = \mathrm{ax}(\dyad{W})\) is defined in terms of the skew-symmetric tensor \(\dyad{W}^\mathrm{T} = -\dyad{W}\) as
\begin{align}\label{eq:id:axial}
\dyad{W} \cdot \vec{u} = \mathrm{ax} (\dyad{W}) \times \vec{u} = \vec{w} \times \vec{u}\,.
\end{align}
To show this, note that the skew tensor \(\vec{a} \otimes \vec{b} - (\vec{a} \otimes \vec{b})^\mathrm{T}\) satsfies
\begin{align*} 
[\vec{a} \otimes \vec{b} - (\vec{a} \otimes \vec{b})^\mathrm{T}] \cdot \vec{u} &= (\vec{b} \cdot \vec{u}) \vec{a} - (\vec{a} \cdot \vec{u}) \vec{b}\notag\\
&= (\vec{b} \times \vec{a}) \times \vec{u}\,, 
\end{align*}
where Item 6 has been used in the second equality. By comparison of the above to Eq. \eqref{eq:id:axial}, it can be seen that 
\[\mathrm{ax}[\vec{a} \otimes \vec{b} - (\vec{a} \otimes \vec{b})^\mathrm{T}] = \vec{b}\times \vec{a}\,.\]
Since any skew-symmetric tensor \(\dyad{W}\) can be built on the basis of \(\vec{a} \otimes \vec{b} - (\vec{a} \otimes \vec{b})^\mathrm{T}\), all skew-symmetric tensors have an axial vector satisfying Eq. \eqref{eq:id:axial}. </p>

<p>Equation \eqref{eq:id:axial} is defined only in terms of a skew-symmetric tensor \(\dyad{W}\). To extend the definition of the axial vector to <i>any</i> tensor \(\dyad{A}\), let
\[\mathrm{ax} \,\dyad{A} = \mathrm{ax} \tfrac{1}{2} (\dyad{A} - \dyad{A}^\mathrm{T})\,.\]
This discussion was adapted from Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>, pp. 18 and Eqs. (1.2.16) and (1.2.17).</p>
</li>

<li> For differentiable tensor and vector fields \(\dyad{T}\) and \(\vec{u}\) in a region \(\mathcal{R}\) with boundary \(\partial R\) having outward normal \(\vec{e}_r\), a consequence of the divergence theorem is
\[\int_{\partial \mathcal{R}} \vec{u} \times (\dyad{T} \cdot \vec{e}_r) \,dA = 
\int_{\mathcal{R}} \{2 \,\mathrm{ax} [\dyad{T} \cdot (\gradient \vec{u})^\mathrm{T}] + \vec{u} \times \divergence \dyad{T} \} \,dV\,,
\]
where the axial vector of a tensor is defined in Item 7. This identity is found in Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a>, Exercise 1.5-2. 
</li>

<li> \((\vec{a} \otimes \vec{b}) \times \vec{w} = \vec{a} \otimes (\vec{b} \times \vec{w})\). According to Ref. <a href="refs.html" target="_blank" rel="noopener noreferrer">[8]</a> [See Eq. (1.4.36)], this identity is in fact definitional and is regarded as the "generalized vector product" (Compare it to the vector triple product in Item 6). Noting that \(\vec{v}\times \vec{w} = -\vec{w} \times \vec{v}\) allows for this identity to be equivalently written as \[\vec{w} \times (\vec{a} \otimes \vec{b})  = - \vec{a} \otimes (\vec{b} \times \vec{w}) = \vec{a} \otimes (\vec{w} \times \vec{b}) \,.\]
</li>


</ol>
<p><a href="index.html">&leftarrow; Return to radiation force homepage</a></p>



</body>
</html>
